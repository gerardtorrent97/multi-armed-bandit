{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Nuclio logo](https://nuclio.school/wp-content/uploads/2018/12/nucleoDS-newBlack.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforced Learning con un multi-armed-bandid usando el algoritmo Epsilon-Greedy \n",
    "\n",
    "El primer ejemplo de reinforced learning que vamos a implementar. La idea es que relleneis los campos a partir de lo explicado en la primera parte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cargamos las librerias que necesitamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inicializamos las variables que queremos usar en el proyecto\n",
    "\n",
    "- Numero de experimentos (pruebas)\n",
    "- Valor de Epsilon\n",
    "- Las probabilidades reales de los bandidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PRUEBAS = 20000\n",
    "PROBABILIDADES_BANDIDOS = [0.2 , 0.5, 0.75] \n",
    "EPS = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer bandido es el peor de todos, es decir, conseguiremos ganar dinero (recordad que son tragaperras) solo un 20% de las veces, cosa que el resto de bandidos son mejores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Definimos nuestra clase Bandido (la tragaperra)\n",
    "\n",
    "Va a tener tres metodos: \n",
    "* El de creación **\\_\\_init\\_\\_** donde definiremos su ratio de exito (p), la media almacenada tras cada experimento (p_estimada), y el número total de veces que le hemos dado a la palanca (N)\n",
    "* El de **ejecución** del experimento .pull() donde calculamos un valor random más la media\n",
    "* El de **actualización** de los valores en caso de exito o fracaso en el experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandit(object):\n",
    "    def __init__(self, p):\n",
    "        # Ratio de exito\n",
    "        self.p = p\n",
    "        # Media almacenada del ratio de exito de cada pull/experimento en un bandido\n",
    "        self.p_estimada = 0.0\n",
    "        # Numero de pulls/experimentos en este bandido\n",
    "        self.N = 0\n",
    "    \n",
    "    def pull(self):\n",
    "        # Devolvemos un booleano (True, False) con una probabilidad p (distribucion de Bernouilli)\n",
    "        return np.random.random() < self.p\n",
    "    \n",
    "    def update(self, x):\n",
    "        # Actualizamos los valores de mean y N, es decir, la media acumulada de exito, más el número de experimentos\n",
    "        # Añadimos un experimento al bandido\n",
    "        self.N  += 1\n",
    "        \n",
    "        # Recalculamos la media del bandido (tras desarrollar la regla de la media)\n",
    "        self.p_estimada += self.p_estimada + (1/self.N) * (int(x) - self.p_estimada)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Definimos el experimento\n",
    "La funcion de realizar el experimento basada en el metodo epsilon-codicioso\n",
    "\n",
    "* Creamos la lista de bandits a partir de los datos iniciales\n",
    "* Almacenamos el número de premios entregados\n",
    "* Inicializamos los contadores de ejecuciones de experimentos\n",
    "\n",
    "Arrancamos un bucle sobre el número de experimentos usando el pseudo-codigo de Epsilon-Greedy\n",
    "\n",
    "* Aplicamos el algoritmo para saber si exploramos o explotamos\n",
    "* Las veces que acertamos el bandido de mejor probabilidad (el optimo)\n",
    "* Tiramos de la palanca del bandit escogido\n",
    "* Actualizamos las recompensas de ese bandit\n",
    "* Almacenamos sus datos\n",
    "\n",
    "Luego pintamos la evolución en el tiempo de las ejecuciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment():\n",
    "    \n",
    "    #Genero las tres tragaperras, los bandidos\n",
    "    bandits = [Bandit(p) for p in PROBABILIDADES_BANDIDOS]\n",
    "    \n",
    "    #Genero un array vacío para la recolección de resultados\n",
    "    premios = np.zeros(NUM_PRUEBAS)\n",
    "    \n",
    "    #Inicializo los contadores de ejecuciones de experimentos\n",
    "    num_times_explored = 0\n",
    "    num_times_exploited = 0\n",
    "    num_optimal = 0\n",
    "    \n",
    "    optimal_j = np.argmax([b.p for b in bandits])\n",
    "       \n",
    "    for i in range(NUM_PRUEBAS):\n",
    "        \n",
    "        # TODO Algoritmo de Epsilon Greedy\n",
    "        \n",
    "        # Cuantas veces hemos escogido el optimo\n",
    "        if jbandit == optimal_j:\n",
    "            num_optimal += 1\n",
    "        \n",
    "        # Con tragaperra escogida, le damos a la palanca haciendo un pull\n",
    "        x = bandits[jbandit].pull()\n",
    "        \n",
    "        # Actualizamos el valor de la media de esa misma tragaperra, recordemos que x es un evento Bernouilli True o False \n",
    "        # Lo hemos de traducir a 1 o 0.\n",
    "        if x:\n",
    "            x = 1\n",
    "        else:\n",
    "            x = 0\n",
    "        bandits[jbandit].update(x)\n",
    "        \n",
    "        # Almacenamos los datos para montar una grafica de evolución en el tiempo\n",
    "        premios[i] = x\n",
    "    \n",
    "    # Calculamos la media acumulada de todos los experimentos\n",
    "    ratio_exito = np.cumsum(premios) / (np.arange(NUM_PRUEBAS)+1)\n",
    "    \n",
    "    # Veamos los resultados\n",
    "    print(\"Total recompensas conseguidas:\", premios.sum())\n",
    "    print(\"Media del ratio de exito:\", premios.sum() / NUM_PRUEBAS)\n",
    "    print(\"Total veces explorado:\", num_times_explored)\n",
    "    print(\"Total veces explotado:\", num_times_exploited)\n",
    "    \n",
    "    \n",
    "    # Número de veces que hemos explotado cada Bandit\n",
    "    for b in bandits:\n",
    "        print(\"Bandit con probabilidad real \", b.p ,\" se ha explotado :\", b.N)\n",
    "    \n",
    "    # Pintamos los resultados\n",
    "    plt.plot(ratio_exito)\n",
    "    plt.plot(np.ones(NUM_PRUEBAS)*np.max(PROBABILIDADES_BANDIDOS))\n",
    "    plt.xscale('log')\n",
    "    plt.show()\n",
    "    \n",
    "    # Imprimimos el resultado en pantalla\n",
    "    for b in bandits:\n",
    "        print(\"Media Estimada de Exito:\", b.p_estimada)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Parte principal del programa\n",
    "Donde ejecutamos los tres experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parte principal del programa\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Ejecutamos tres experimentos con distintos Epsilon\n",
    "    run_experiment()\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd07077c909d86029f482cfb7761bb0f65301604cf41dd63eb35afb2938078adec0",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}